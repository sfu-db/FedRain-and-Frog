{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.metrics import roc_curve\n",
    "# from utilities import *\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/propublica/compas-analysis/raw/master/compas-scores-two-years.csv\", \n",
    "                 header=0).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_compas(df):\n",
    "    \n",
    "    # Clean the compas dataset according to the description provided by ProPublica of their analysis. \n",
    "    # In the original notebook the authors state:\n",
    "\n",
    "    # There are a number of reasons remove rows because of missing data:\n",
    "        \n",
    "        # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, \n",
    "        # we assume that because of data quality reasons, that we do not have the right offense.\n",
    "\n",
    "        # We coded the recidivist flag -- `is_recid` -- to be -1 if we could not find a compas case at all.\n",
    "\n",
    "        # In a similar vein, ordinary traffic offenses -- those with a `c_charge_degree` of 'O' -- will not result in Jail time \n",
    "        # are removed (only two of them).\n",
    " \n",
    "        # We filtered the underlying data from Broward county to include only those rows representing people who had either \n",
    "        # recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "\n",
    "    # ix is the index of variables we want to keep.\n",
    "    # Remove entries with inconsistent arrest information.\n",
    "    rows_start = len(df)\n",
    "    ix = df['days_b_screening_arrest'] <= 30\n",
    "    ix = (df['days_b_screening_arrest'] >= -30) & ix\n",
    "\n",
    "    # remove entries entries where compas case could not be found.\n",
    "    ix = (df['is_recid'] != -1) & ix\n",
    "\n",
    "    # remove traffic offenses.\n",
    "    ix = (df['c_charge_degree'] != \"O\") & ix\n",
    "\n",
    "    # remove entries without available text scores.\n",
    "    ix = (df['score_text'] != 'N/A') & ix\n",
    "\n",
    "    # trim dataset\n",
    "    df = df.loc[ix,:]\n",
    "\n",
    "    # create new attribute \"length of stay\" with total jail time.\n",
    "    df['length_of_stay'] = (pd.to_datetime(df['c_jail_out'])-pd.to_datetime(df['c_jail_in'])).apply(lambda x: x.days)\n",
    "\n",
    "    # print number of rows\n",
    "    print('Number of rows removed: '+str(rows_start - len(df)))\n",
    "    # print list of features again\n",
    "    print('Features: '+str(list(df)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count', 'days_b_screening_arrest', 'decile_score', 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out']\n",
      "Number of rows removed: 1042\n",
      "Features: ['age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count', 'days_b_screening_arrest', 'decile_score', 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out', 'length_of_stay']\n",
      "\n",
      "dataset shape (rows, columns) (6172, 14)\n"
     ]
    }
   ],
   "source": [
    "print(list(df))\n",
    "df.head()\n",
    "# Select features that will be analyzed\n",
    "features_to_keep = ['age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count', \n",
    "                    'days_b_screening_arrest', 'decile_score', 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out']\n",
    "df = df[features_to_keep]\n",
    "df = clean_compas(df)\n",
    "df.head()\n",
    "print(\"\\ndataset shape (rows, columns)\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['race'].isin(['African-American','Caucasian'])]\n",
    "dfQ = df.copy()\n",
    "\n",
    "# Quantize priors count between 0, 1-3, and >3\n",
    "def quantizePrior(x):\n",
    "    if x <=0:\n",
    "        return '0'\n",
    "    elif 1<=x<=3:\n",
    "        return '1 to 3'\n",
    "    else:\n",
    "        return 'More than 3'\n",
    "\n",
    "    \n",
    "# Quantize length of stay\n",
    "def quantizeLOS(x):\n",
    "    if x<= 7:\n",
    "        return '<week'\n",
    "    if 8<x<=93:\n",
    "        return '<3months'\n",
    "    else:\n",
    "        return '>3 months'\n",
    "    \n",
    "# Quantize length of stay\n",
    "def adjustAge(x):\n",
    "    if x == '25 - 45':\n",
    "        return '25 to 45'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# Quantize score_text to MediumHigh\n",
    "def quantizeScore(x):\n",
    "    if (x == 'High')| (x == 'Medium'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "dfQ['priors_count'] = dfQ['priors_count'].apply(quantizePrior)\n",
    "dfQ['length_of_stay'] = dfQ['length_of_stay'].apply(quantizeLOS)\n",
    "dfQ['score_text'] = dfQ['score_text'].apply(quantizeScore)\n",
    "dfQ['age_cat'] = dfQ['age_cat'].apply(adjustAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dfQ.to_csv('compas.csv',index=False, header=True)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#             x, y, test_size=0.2, random_state=random_state,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>race</th>\n",
       "      <th>score_text</th>\n",
       "      <th>African-American</th>\n",
       "      <th>Caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1346</td>\n",
       "      <td>1407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1829</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "race  score_text  African-American  Caucasian\n",
       "0              0              1346       1407\n",
       "1              1              1829        696"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Let's measure the disparate impact according to the EEOC rule\n",
    "means_score = dfQ.groupby(['score_text','race']).size().unstack().reset_index()\n",
    "# means_score = means_score/means_score.sum()\n",
    "means_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race\n",
       "score_text             1\n",
       "African-American    3175\n",
       "Caucasian           2103\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_score.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1829"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute disparte impact\n",
    "AA_with_high_score = means_score.loc[1,'African-American']\n",
    "C_with_high_score = means_score.loc[1,'Caucasian']\n",
    "\n",
    "# C_with_high_score/AA_with_high_score\n",
    "AA_with_high_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "\n",
    "# import gurobipy\n",
    "# from json import dumps, loads\n",
    "# from time import time\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.linear_model import LogisticRegression as skLogisticRegression\n",
    "# from sklearn.metrics import (classification_report, f1_score, precision_score, recall_score)\n",
    "# from tqdm import tnrange, trange\n",
    "# import tensorflow as tf\n",
    "\n",
    "# from mlsql.influence import InfluenceRanker\n",
    "# from mlsql.fixer import AutoFixer\n",
    "# from mlsql.manager import ModelManagerLM\n",
    "# from mlsql.manager_test import ModelManagerTest\n",
    "\n",
    "# from models.simple_cnn import SimpleCNN\n",
    "# from models.logreg import LogReg\n",
    "# from models.linear_comb import LinearComb\n",
    "# from models.linear_comb_test import LinearCombTest\n",
    "# from processors.compas import CompasProcessor\n",
    "\n",
    "\n",
    "# import logging\n",
    "# logging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# import time\n",
    "# import altair as alt\n",
    "# alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>race</th>\n",
       "      <th>score_text</th>\n",
       "      <th>African-American</th>\n",
       "      <th>Caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.423937</td>\n",
       "      <td>0.669044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.576063</td>\n",
       "      <td>0.330956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "race  score_text  African-American  Caucasian\n",
       "0            0.0          0.423937   0.669044\n",
       "1            1.0          0.576063   0.330956"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race\n",
       "score_text          1.0\n",
       "African-American    1.0\n",
       "Caucasian           1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
