{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from processor import NLProcessor\n",
    "\n",
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_rate           = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = NLProcessor()\n",
    "processor.load_spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=1, solver = \"lbfgs\",\n",
    "                           max_iter = 800,\n",
    "                           fit_intercept= False,\n",
    "                           warm_start = True)\n",
    "\n",
    "def evaluate(model):\n",
    "    predictions = model.predict(processor.X_test)\n",
    "    return accuracy_score(processor.Y_test, predictions)\n",
    "    \n",
    "    \n",
    "accuracy_scores = []\n",
    "guess_scores  = []\n",
    "for seed in range(15):\n",
    "\n",
    "    X_train,  ycrptd = processor.corrupt_random(seed, corrupt_rate)\n",
    "    model.fit(X_train, ycrptd)\n",
    "    \n",
    "    # Let us calculate the initial score of the model\n",
    "    initial_score = evaluate(model)\n",
    "    accuracy_scores.append([initial_score])\n",
    "    guess_scores.append([0])\n",
    "    \n",
    "    # Let us make some rounds of correction of sample based on training loss\n",
    "    correction_rounds = int(2 * X_train.shape[0] * corrupt_rate)\n",
    "    \n",
    "    # Get influences\n",
    "    influences = processor.training_influence(model, ycrptd, C = 1)\n",
    "    order = np.argsort(influences)\n",
    "    \n",
    "    for i in tnrange(correction_rounds):\n",
    "        to_check = order[i]\n",
    "        if ycrptd[to_check] == processor.Y_train[to_check]:\n",
    "            # The guess was incorect. No need to retrain the model\n",
    "            score = accuracy_scores[-1][-1]\n",
    "            correct_guess = 0\n",
    "        else:\n",
    "            # It was a good guess\n",
    "            ycrptd[to_check] = processor.Y_train[to_check]\n",
    "            model.fit(X_train, ycrptd)\n",
    "            score = evaluate(model)\n",
    "            correct_guess = 1\n",
    "        \n",
    "        accuracy_scores[-1].append(score)\n",
    "        guess_scores[-1].append(guess_scores[-1][-1] + correct_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_array = np.array(accuracy_scores)\n",
    "guess_array  = np.array(guess_scores)\n",
    "\n",
    "accuracy_means = np.mean(accuracy_array[:, ::15], axis = 0)\n",
    "accuracy_mins  = np.min(accuracy_array[:, ::15], axis  = 0)\n",
    "accuracy_maxs  = np.max(accuracy_array[:, ::15], axis  = 0)\n",
    "stacked      = np.stack([accuracy_means -accuracy_mins, accuracy_maxs- accuracy_means])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.errorbar(range(len(accuracy_array[0]))[::15], accuracy_means, yerr = stacked)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_means = np.mean(guess_array[:, ::15]/(X_train.shape[0] * corrupt_rate), axis = 0)\n",
    "guess_mins  = np.min(guess_array[:, ::15]/(X_train.shape[0] * corrupt_rate), axis  = 0)\n",
    "guess_maxs  = np.max(guess_array[:, ::15]/(X_train.shape[0] * corrupt_rate), axis  = 0)\n",
    "stacked      = np.stack([guess_means -guess_mins, guess_maxs- guess_means])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.errorbar(range(len(guess_array[0]))[::15], guess_means, yerr = stacked, label = 'Influence Correction')\n",
    "plt.plot(range(len(guess_array[0]))[:int(X_train.shape[0] * corrupt_rate):15], \n",
    "         np.linspace(0, 1, int(X_train.shape[0] * corrupt_rate))[::15],  label = 'Oracle')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_means_inf = np.mean(guess_array/(X_train.shape[0] * corrupt_rate), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store guess_means_inf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
