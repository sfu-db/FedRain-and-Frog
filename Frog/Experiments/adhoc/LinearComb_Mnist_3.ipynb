{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import gurobipy\n",
    "from json import dumps, loads\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression as skLogisticRegression\n",
    "from sklearn.metrics import (classification_report, f1_score, precision_score, recall_score)\n",
    "from tqdm import tnrange, trange\n",
    "import tensorflow as tf\n",
    "\n",
    "from mlsql.influence import InfluenceRanker\n",
    "from mlsql.fixer import AutoFixer\n",
    "from mlsql.manager import ModelManagerLM\n",
    "from mlsql.manager_test import ModelManagerTest\n",
    "\n",
    "from models.simple_cnn import SimpleCNN\n",
    "from models.logreg import LogReg\n",
    "from models.linear_comb import LinearComb\n",
    "from models.linear_comb_test import LinearCombTest\n",
    "from processors.mnistbinary import MnistBinaryProcessor\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\n",
    "\n",
    "import time\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def rank_fix(ranker, fixer, n):\n",
    "    rank = ranker.predict()\n",
    "    fixer.fix(rank, n)\n",
    "    return rank\n",
    "\n",
    "@tf.function\n",
    "def rankit(ranker):\n",
    "    rank = ranker.predict()\n",
    "    return rank\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def fixit(fixer, rank, n):\n",
    "    fixer.fix(rank, n)\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "def train(manager):\n",
    "    manager.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = MnistBinaryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD loss: tf.Tensor(0.45520002, shape=(), dtype=float32)\n",
      "SGD steps: 888\n",
      "160.40223050117493\n",
      "Model name: LogReg\n",
      "On Training\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82     34810\n",
      "         1.0       0.68      0.68      0.68     19190\n",
      "\n",
      "    accuracy                           0.77     54000\n",
      "   macro avg       0.75      0.75      0.75     54000\n",
      "weighted avg       0.77      0.77      0.77     54000\n",
      "\n",
      "On Testing\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.98      0.85      4926\n",
      "         1.0       0.97      0.69      0.81      5074\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.86      0.83      0.83     10000\n",
      "weighted avg       0.86      0.83      0.83     10000\n",
      "\n",
      "Model name: LogReg\n",
      "On Training\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82     34810\n",
      "         1.0       0.68      0.68      0.68     19190\n",
      "\n",
      "    accuracy                           0.77     54000\n",
      "   macro avg       0.75      0.75      0.75     54000\n",
      "weighted avg       0.77      0.77      0.77     54000\n",
      "\n",
      "On Testing\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.97      0.85      2905\n",
      "         1.0       0.97      0.70      0.81      3095\n",
      "\n",
      "    accuracy                           0.83      6000\n",
      "   macro avg       0.86      0.83      0.83      6000\n",
      "weighted avg       0.86      0.83      0.83      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogReg(1)\n",
    "manager0 = ModelManagerLM(proc.x_train, proc.y_corr, model, 256)\n",
    "start = time.time()\n",
    "manager0.fit(print_value=True, tol=1e-5, lr=0.1, max_iter=2000)\n",
    "print(time.time() - start)\n",
    "manager0.report(proc.x_train, proc.y_corr, proc.x_test, proc.y_test)\n",
    "manager0.report(proc.x_train, proc.y_corr, proc.x_query, proc.y_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8223\n"
     ]
    }
   ],
   "source": [
    "K = 8223\n",
    "corrsel = proc.corrsel\n",
    "print(len(list(np.where(corrsel)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46549d0395c440299adbb4a7f3f7a407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank_time: 551.1078689098358\n",
      "Model_time: 1.6689300537109375e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-cc37d7a6e6984c8094745b2b244f8a57\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-cc37d7a6e6984c8094745b2b244f8a57\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-cc37d7a6e6984c8094745b2b244f8a57\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-625bba8385e7e9deea36c3e52bd48ce8\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Method\"}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"grid\": false, \"tickCount\": 2}, \"field\": \"K\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Complain\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-625bba8385e7e9deea36c3e52bd48ce8\": [{\"Complain\": -951.369873046875, \"F1\": 0.8275686027754373, \"K\": 0, \"Method\": \"Rain\"}, {\"Complain\": -951.369873046875, \"F1\": 0.8275686027754373, \"K\": 8223, \"Method\": \"Rain\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tnrange, trange\n",
    "manager = ModelManagerLM(proc.x_train, proc.y_corr, LogReg(1), 256)\n",
    "manager.model.set_weights(manager0.model.get_weights())\n",
    "manager.delta = tf.Variable(manager0.delta.value(), name=\"delta\")\n",
    "ranker = InfluenceRanker(manager=manager, on=proc.complain)\n",
    "fixer = AutoFixer(manager, corrsel, K)\n",
    "\n",
    "AQs = []\n",
    "weighted_f1 = []\n",
    "rank_list = []\n",
    "rank_time_rain = 0\n",
    "model_time_rain = 0\n",
    "AQ = proc.complain(manager).AQ\n",
    "# f1 = f1_score(proc.y_test.numpy(), manager.model.predict(proc.x_test).numpy(), average='weighted')\n",
    "f1 = f1_score(proc.y_query.numpy(), manager.model.predict(proc.x_query).numpy(), average='weighted')\n",
    "AQs.append(float(AQ))\n",
    "weighted_f1.append(f1)\n",
    "\n",
    "step_size = 8223\n",
    "rain_k = int(np.ceil(K / step_size))\n",
    "for k in trange(0, rain_k):\n",
    "    nfix = min(step_size, K - step_size * k)\n",
    "    assert nfix > 0\n",
    "\n",
    "    start = time.time()\n",
    "    rank = rank_fix(ranker, fixer, nfix)\n",
    "    middle = time.time()\n",
    "#     manager.fit(max_iter=1000, print_value=True, lr=0.1, tol=1e-5)\n",
    "    end = time.time()\n",
    "    \n",
    "    rank_list.append(rank.numpy())\n",
    "    rank_time_rain += middle - start\n",
    "    model_time_rain += end - middle\n",
    "\n",
    "    AQ = proc.complain(manager).AQ\n",
    "#     f1 = f1_score(proc.y_test.numpy(), manager.model.predict(proc.x_test).numpy(), average='weighted')\n",
    "    f1 = f1_score(proc.y_query.numpy(), manager.model.predict(proc.x_query).numpy(), average='weighted')\n",
    "    AQs.append(float(AQ))\n",
    "    weighted_f1.append(f1)\n",
    "\n",
    "print(\"Rank_time:\", rank_time_rain)\n",
    "print(\"Model_time:\", model_time_rain)\n",
    "AC = proc.complain(manager).AC\n",
    "\n",
    "df_rain = pd.DataFrame({\n",
    "    \"Complain\": np.array(AQs) - AC,\n",
    "    \"F1\": np.array(weighted_f1),\n",
    "    \"K\": list(range(0, K, step_size)) + [K],\n",
    "    \"Method\": np.repeat(\"Rain\", len(AQs)),\n",
    "})\n",
    "alt.Chart(pd.concat([df_rain])).mark_line().encode(\n",
    "    alt.X('K:Q', axis=alt.Axis(tickCount=df_rain.shape[0], grid=False)),\n",
    "    alt.Y(\"Complain:Q\"),\n",
    "    color = \"Method\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.21610118e-04, 1.12124529e-01, 2.23032956e-01, 3.28712149e-01,\n",
       "       4.29526937e-01, 5.22558677e-01, 6.07320929e-01, 6.83570473e-01,\n",
       "       7.42916211e-01, 7.55563663e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((fixer.recall_k()[0::1000], fixer.recall_k()[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD loss: tf.Tensor(0.070950076, shape=(), dtype=float32)\n",
      "SGD steps: 61585\n",
      "7602.085188627243\n",
      "Model name: LinearCombTest\n",
      "On Training\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.77      0.83     34810\n",
      "         1.0       0.67      0.83      0.74     19190\n",
      "\n",
      "    accuracy                           0.79     54000\n",
      "   macro avg       0.78      0.80      0.79     54000\n",
      "weighted avg       0.81      0.79      0.80     54000\n",
      "\n",
      "On Testing\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.96      0.90      4926\n",
      "         1.0       0.95      0.83      0.89      5074\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.90      0.89      0.89     10000\n",
      "weighted avg       0.90      0.89      0.89     10000\n",
      "\n",
      "Model name: LinearCombTest\n",
      "On Training\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.77      0.83     34810\n",
      "         1.0       0.67      0.83      0.74     19190\n",
      "\n",
      "    accuracy                           0.79     54000\n",
      "   macro avg       0.78      0.80      0.79     54000\n",
      "weighted avg       0.81      0.79      0.80     54000\n",
      "\n",
      "On Testing\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.95      0.90      2905\n",
      "         1.0       0.95      0.84      0.89      3095\n",
      "\n",
      "    accuracy                           0.89      6000\n",
      "   macro avg       0.90      0.90      0.89      6000\n",
      "weighted avg       0.90      0.89      0.89      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LinearCombTest\n",
    "model = LinearCombTest(1)\n",
    "manager_test0 = ModelManagerTest(proc.x_a_train, proc.x_b_train, proc.y_corr, model, 256)\n",
    "start = time.time()\n",
    "manager_test0.fit(print_value=True, tol=1e-10, lr=0.5, max_iter=100000)\n",
    "print(time.time() - start)\n",
    "manager_test0.report(proc.x_a_train, proc.x_b_train, proc.y_corr, proc.x_a_test, proc.x_b_test, proc.y_test)\n",
    "manager_test0.report(proc.x_a_train, proc.x_b_train, proc.y_corr, proc.x_a_query, proc.x_b_query, proc.y_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_test = ModelManagerTest(proc.x_a_train, proc.x_b_train, proc.y_corr, LinearCombTest(1), 256)\n",
    "manager_test.model.set_weights(manager_test0.model.get_weights())\n",
    "manager_test.delta = tf.Variable(manager_test0.delta.value(), name=\"delta\")\n",
    "ranker = InfluenceRanker(manager=manager_test, on=proc.test_complain)\n",
    "fixer = AutoFixer(manager_test, proc.corrsel, K)\n",
    "\n",
    "AQs = []\n",
    "weighted_f1 = []\n",
    "rank_list = []\n",
    "rank_time_rain = 0\n",
    "model_time_rain = 0\n",
    "AQ = proc.test_complain(manager_test).AQ\n",
    "# f1 = f1_score(proc.y_test.numpy(), manager.model.predict(proc.x_test).numpy(), average='weighted')\n",
    "f1 = f1_score(proc.y_query.numpy(), manager_test.model.predict(proc.x_a_query, proc.x_b_query).numpy(), average='weighted')\n",
    "AQs.append(float(AQ))\n",
    "weighted_f1.append(f1)\n",
    "\n",
    "step_size = 1000\n",
    "rain_k = int(np.ceil(K / step_size))\n",
    "for k in trange(0, rain_k):\n",
    "    nfix = min(step_size, K - step_size * k)\n",
    "    assert nfix > 0\n",
    "\n",
    "    start = time.time()\n",
    "    rank = rank_fix(ranker, fixer, nfix)\n",
    "    middle = time.time()\n",
    "    manager_test.fit(max_iter=5000, tol=1e-8, lr=0.1, print_value=True)\n",
    "    end = time.time()\n",
    "    \n",
    "    rank_list.append(rank.numpy())\n",
    "    rank_time_rain += middle - start\n",
    "    model_time_rain += end - middle\n",
    "\n",
    "    AQ = proc.test_complain(manager_test).AQ\n",
    "#     f1 = f1_score(proc.y_test.numpy(), manager.model.predict(proc.x_test).numpy(), average='weighted')\n",
    "    f1 = f1_score(proc.y_query.numpy(), manager_test.model.predict(proc.x_a_query, proc.x_b_query).numpy(), average='weighted')\n",
    "    AQs.append(float(AQ))\n",
    "    weighted_f1.append(f1)\n",
    "\n",
    "print(\"Rank_time:\", rank_time_rain)\n",
    "print(\"Model_time:\", model_time_rain)\n",
    "AC = proc.test_complain(manager_test).AC\n",
    "\n",
    "df_rain_test = pd.DataFrame({\n",
    "    \"Complain\": np.array(AQs) - AC,\n",
    "    \"F1\": np.array(weighted_f1),\n",
    "    \"K\": list(range(0, K, step_size)) + [K],\n",
    "    \"Method\": np.repeat(\"Rain\", len(AQs)),\n",
    "})\n",
    "alt.Chart(pd.concat([df_rain_test])).mark_line().encode(\n",
    "    alt.X('K:Q', axis=alt.Axis(tickCount=df_rain_test.shape[0], grid=False)),\n",
    "    alt.Y(\"Complain:Q\"),\n",
    "    color = \"Method\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b4067fc88443c88e0dde65a9efdb66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank_time: 505.00672936439514\n",
      "Model_time: 2.1457672119140625e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-377376d01d8e433a8cbcacca85b1b482\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-377376d01d8e433a8cbcacca85b1b482\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-377376d01d8e433a8cbcacca85b1b482\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-810b9f246f1444d77151c41cde6941db\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Method\"}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"grid\": false, \"tickCount\": 2}, \"field\": \"K\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Complain\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-810b9f246f1444d77151c41cde6941db\": [{\"Complain\": -948.484130859375, \"F1\": 0.8934665049400824, \"K\": 0, \"Method\": \"Rain\"}, {\"Complain\": -948.484130859375, \"F1\": 0.8934665049400824, \"K\": 8223, \"Method\": \"Rain\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager_test = ModelManagerTest(proc.x_a_train, proc.x_b_train, proc.y_corr, LinearCombTest(1), 256)\n",
    "manager_test.model.set_weights(manager_test0.model.get_weights())\n",
    "manager_test.delta = tf.Variable(manager_test0.delta.value(), name=\"delta\")\n",
    "ranker = InfluenceRanker(manager=manager_test, on=proc.test_complain)\n",
    "fixer = AutoFixer(manager_test, proc.corrsel, K)\n",
    "\n",
    "AQs = []\n",
    "weighted_f1 = []\n",
    "rank_list = []\n",
    "rank_time_rain = 0\n",
    "model_time_rain = 0\n",
    "AQ = proc.test_complain(manager_test).AQ\n",
    "# f1 = f1_score(proc.y_test.numpy(), manager.model.predict(proc.x_test).numpy(), average='weighted')\n",
    "f1 = f1_score(proc.y_query.numpy(), manager_test.model.predict(proc.x_a_query, proc.x_b_query).numpy(), average='weighted')\n",
    "AQs.append(float(AQ))\n",
    "weighted_f1.append(f1)\n",
    "\n",
    "step_size = 8223\n",
    "rain_k = int(np.ceil(K / step_size))\n",
    "for k in trange(0, rain_k):\n",
    "    nfix = min(step_size, K - step_size * k)\n",
    "    assert nfix > 0\n",
    "\n",
    "    start = time.time()\n",
    "    rank = rank_fix(ranker, fixer, nfix)\n",
    "    middle = time.time()\n",
    "#     manager_test.fit(max_iter=5000, tol=1e-8, lr=0.1, print_value=True)\n",
    "    end = time.time()\n",
    "    \n",
    "    rank_list.append(rank.numpy())\n",
    "    rank_time_rain += middle - start\n",
    "    model_time_rain += end - middle\n",
    "\n",
    "    AQ = proc.test_complain(manager_test).AQ\n",
    "#     f1 = f1_score(proc.y_test.numpy(), manager.model.predict(proc.x_test).numpy(), average='weighted')\n",
    "    f1 = f1_score(proc.y_query.numpy(), manager_test.model.predict(proc.x_a_query, proc.x_b_query).numpy(), average='weighted')\n",
    "    AQs.append(float(AQ))\n",
    "    weighted_f1.append(f1)\n",
    "\n",
    "print(\"Rank_time:\", rank_time_rain)\n",
    "print(\"Model_time:\", model_time_rain)\n",
    "AC = proc.test_complain(manager_test).AC\n",
    "\n",
    "df_rain_test = pd.DataFrame({\n",
    "    \"Complain\": np.array(AQs) - AC,\n",
    "    \"F1\": np.array(weighted_f1),\n",
    "    \"K\": list(range(0, K, step_size)) + [K],\n",
    "    \"Method\": np.repeat(\"Rain\", len(AQs)),\n",
    "})\n",
    "alt.Chart(pd.concat([df_rain_test])).mark_line().encode(\n",
    "    alt.X('K:Q', axis=alt.Axis(tickCount=df_rain_test.shape[0], grid=False)),\n",
    "    alt.Y(\"Complain:Q\"),\n",
    "    color = \"Method\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.08707284, 0.19275204, 0.29247233, 0.39243585,\n",
       "       0.48996717, 0.58336374, 0.66508574, 0.73768698, 0.75337468])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((fixer.recall_k()[0::1000], fixer.recall_k()[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
